{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.0.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphs의 결과 : 형태소 추출\n",
      "['죽다', '직전', '열리다', '자정', '의', '도서관', '에서', '노', '라는', '자신', '이', '선택', '하다', '않다', '또', '다른', '자신', '의', '삶', '을', '살', '아', '볼', '기회', '를', '얻다']\n",
      "nouns의 결과 : 명사 추출\n",
      "['직전', '자정', '도서관', '노', '자신', '선택', '또', '다른', '자신', '삶', '살', '볼', '기회']\n",
      "phrases의 결과 : 어절 추출\n",
      "['직전', '직전 열리는 자정', '직전 열리는 자정의 도서관', '자신', '선택', '또 다른', '또 다른 자신', '또 다른 자신의 삶', '볼 기회', '자정', '도서관', '다른', '기회']\n",
      "pos의 결과 : 형태소로 나눈 뒤 그에 해당하는 품사를 태깅\n",
      "정규화O 어간추출O\n",
      "[('죽다', 'Verb'), ('직전', 'Noun'), ('열리다', 'Verb'), ('자정', 'Noun'), ('의', 'Josa'), ('도서관', 'Noun'), ('에서', 'Josa'), ('노', 'Noun'), ('라는', 'Josa'), ('자신', 'Noun'), ('이', 'Josa'), ('선택', 'Noun'), ('하다', 'Verb'), ('않다', 'Verb'), ('또', 'Noun'), ('다른', 'Noun'), ('자신', 'Noun'), ('의', 'Josa'), ('삶', 'Noun'), ('을', 'Josa'), ('살', 'Noun'), ('아', 'Josa'), ('볼', 'Noun'), ('기회', 'Noun'), ('를', 'Josa'), ('얻다', 'Verb')]\n",
      "정규화X 어간추출O\n",
      "[('죽다', 'Verb'), ('직전', 'Noun'), ('열리다', 'Verb'), ('자정', 'Noun'), ('의', 'Josa'), ('도서관', 'Noun'), ('에서', 'Josa'), ('노', 'Noun'), ('라는', 'Josa'), ('자신', 'Noun'), ('이', 'Josa'), ('선택', 'Noun'), ('하다', 'Verb'), ('않다', 'Verb'), ('또', 'Noun'), ('다른', 'Noun'), ('자신', 'Noun'), ('의', 'Josa'), ('삶', 'Noun'), ('을', 'Josa'), ('살', 'Noun'), ('아', 'Josa'), ('볼', 'Noun'), ('기회', 'Noun'), ('를', 'Josa'), ('얻다', 'Verb')]\n",
      "정규화O 어간추출X\n",
      "[('죽기', 'Verb'), ('직전', 'Noun'), ('열리는', 'Verb'), ('자정', 'Noun'), ('의', 'Josa'), ('도서관', 'Noun'), ('에서', 'Josa'), ('노', 'Noun'), ('라는', 'Josa'), ('자신', 'Noun'), ('이', 'Josa'), ('선택', 'Noun'), ('하지', 'Verb'), ('않았던', 'Verb'), ('또', 'Noun'), ('다른', 'Noun'), ('자신', 'Noun'), ('의', 'Josa'), ('삶', 'Noun'), ('을', 'Josa'), ('살', 'Noun'), ('아', 'Josa'), ('볼', 'Noun'), ('기회', 'Noun'), ('를', 'Josa'), ('얻는다', 'Verb')]\n",
      "정규화X 어간추출X\n",
      "[('죽기', 'Verb'), ('직전', 'Noun'), ('열리는', 'Verb'), ('자정', 'Noun'), ('의', 'Josa'), ('도서관', 'Noun'), ('에서', 'Josa'), ('노', 'Noun'), ('라는', 'Josa'), ('자신', 'Noun'), ('이', 'Josa'), ('선택', 'Noun'), ('하지', 'Verb'), ('않았던', 'Verb'), ('또', 'Noun'), ('다른', 'Noun'), ('자신', 'Noun'), ('의', 'Josa'), ('삶', 'Noun'), ('을', 'Josa'), ('살', 'Noun'), ('아', 'Josa'), ('볼', 'Noun'), ('기회', 'Noun'), ('를', 'Josa'), ('얻는다', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "text=\"죽기 직전 열리는 자정의 도서관에서 노라는 자신이 선택하지 않았던 또 다른 자신의 삶을 살아볼 기회를 얻는다\"\n",
    "print('morphs의 결과 : 형태소 추출')\n",
    "print(okt.morphs(text,stem=True))\n",
    "print(\"nouns의 결과 : 명사 추출\")\n",
    "print(okt.nouns(text))\n",
    "print(\"phrases의 결과 : 어절 추출\")\n",
    "print(okt.phrases(text))\n",
    "print(\"pos의 결과 : 형태소로 나눈 뒤 그에 해당하는 품사를 태깅\")\n",
    "print(\"정규화O 어간추출O\")\n",
    "print(okt.pos(text,norm=True,stem=True))\n",
    "print(\"정규화X 어간추출O\")\n",
    "print(okt.pos(text,norm=False,stem=True))\n",
    "print(\"정규화O 어간추출X\")\n",
    "print(okt.pos(text,norm=True,stem=False))\n",
    "print(\"정규화X 어간추출X\")\n",
    "print(okt.pos(text,norm=False,stem=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('죽다', 'Verb'), ('직전', 'Noun'), ('열리다', 'Verb'), ('자정', 'Noun'), ('의', 'Josa'), ('도서관', 'Noun'), ('에서', 'Josa'), ('노', 'Noun'), ('라는', 'Josa'), ('자신', 'Noun'), ('이', 'Josa'), ('선택', 'Noun'), ('하다', 'Verb'), ('않다', 'Verb'), ('또', 'Noun'), ('다른', 'Noun'), ('자신', 'Noun'), ('의', 'Josa'), ('삶', 'Noun'), ('을', 'Josa'), ('살', 'Noun'), ('아', 'Josa'), ('볼', 'Noun'), ('기회', 'Noun'), ('를', 'Josa'), ('얻다', 'Verb')]\n",
      "<class 'str'>\n",
      "['죽다', '직전', '열리다', '자정', '의', '도서관', '에서', '노', '라는', '자신', '이', '선택', '하다', '않다', '또', '다른', '자신', '의', '삶', '을', '살', '아', '볼', '기회', '를', '얻다']\n"
     ]
    }
   ],
   "source": [
    "clean_review=okt.pos(text,norm=True,stem=True)\n",
    "print(clean_review)\n",
    "print(type(clean_review[0][1]))\n",
    "for i in range(len(clean_review)):\n",
    "    clean_review[i]=clean_review[i][0]\n",
    "print(clean_review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTLK는 한국어 불용어 데이터를 제공하지 않습니다. \n",
    "#왜냐하면 토큰화 단계에서 필요없는 조사, 접속사를 제거하면 되기 때문입니다.\n",
    "#그러므로 불용어를 수동적으로 지정하여 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NTLK는', '한국어', '불용어', '데이터를', '제공하지', '않습니다', '.', '왜냐하면', '토큰화', '단계에서', '필요없는', '조사', ',', '접속사를', '제거하면', '되기', '때문입니다', '.'] \n",
      "\n",
      "['NTLK는', '한국어', '불용어', '데이터를', '제공하지', '토큰화', '단계에서', '필요없는', '조사', '접속사를', '제거하면', '때문입니다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"NTLK는 한국어 불용어 데이터를 제공하지 않습니다. 왜냐하면 토큰화 단계에서 필요없는 조사, 접속사를 제거하면 되기 때문입니다.\" \n",
    "stop_words = \"않습니다 왜냐하면 되기 를 은 , . 하는 이 가 은 는 하는 왜냐하면 하기\" \n",
    "word_tokens = word_tokenize(example) \n",
    "stop_words=stop_words.split(' ') \n",
    "result = [] \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "print(word_tokens,'\\n') \n",
    "print(result)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자, 정리하자면 자연어를 처리할 때 즉 한글 데이터를 정제할 때 필요한 것\n",
    "- BeautifulSoup을 통한 HTML 태그 제거\n",
    "- 정규화로 표준 한글이 아닌 경우는 공백으로 치환\n",
    "- NLTK 데이터를 사용해 불용어(Stopword) 제거 (다만, NLTK는 한글을 지원하지 않으므로 직접 불용어 리스트를 정의한 후 한국어 불용어를 제거해주면 된다.)\n",
    "- 어간추출(Stemming)\n",
    "- 음소 표기법(lemmatization) 동음이의어가 문맥에 따라 다른 의미를 가질 때 앞뒤 문맥을 보고 단어의 의미를 식별하는것(이건 한글로 어떻게 하는지 모르겠음????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = \"않습니다 왜냐하면 되기 를 은 , . 하는 이 가 은 는 하는 왜냐하면 하기\" \n",
    "stop_words=stop_words.split(' ') \n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. HTML 제거\n",
    "    #review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
    "    # stopwords 를 세트로 변환한다.\n",
    "    #stops = set(setstop_words)\n",
    "    # 5. Stopwords 불용어 제거\n",
    "    #meaningful_words = [w for w in words if not w in stops]\n",
    "    # 6. 어간추출\n",
    "    #stemming_words = [stemmer.stem(w) for w in meaningful_words]\n",
    "    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환\n",
    "    return( ' '.join(raw_review) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['직', '직', '직', '자', '선', '볼', '자', '도', '다', '기']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'직 직 직 자 선 또 또 또 볼 자 도 다 기'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"죽기 직전 열리는 자정의 도서관에서 노라는 자신이 선택하지 않았던 또 다른 자신의 삶을 살아볼 기회를 얻는다.\"\n",
    "text=okt.phrases(text)\n",
    "for i in range(len(text)):\n",
    "    text[i]=text[i][0]\n",
    "stop_words = \"않습니다 왜냐하면 되기 를 은 , . 노 하 하는 이 가 은 는 하는 왜냐하면 하기 에서 라는 의 하다 않다 또\" \n",
    "stop_words=stop_words.split(' ') \n",
    "stops=set(stop_words)\n",
    "meaningful_words = [w for w in text if not w in stops]\n",
    "print(meaningful_words)\n",
    "review_to_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['죽다', '직전', '열리다', '자정', '도서관', '자신', '선택', '다른', '자신', '삶', '을', '살', '아', '볼', '기회', '얻다']\n"
     ]
    }
   ],
   "source": [
    "text=\"죽기 직전 열리는 자정의 도서관에서 노라는 자신이 선택하지 않았던 또 다른 자신의 삶을 살아볼 기회를 얻는다.\"\n",
    "text=okt.pos(text,norm=True,stem=True)\n",
    "for i in range(len(text)):\n",
    "    text[i]=text[i][0]\n",
    "stop_words = \"않습니다 왜냐하면 되기 를 은 , . 노 하 하는 이 가 은 는 하는 왜냐하면 하기 에서 라는 의 하다 않다 또\" \n",
    "stop_words=stop_words.split(' ') \n",
    "stops=set(stop_words)\n",
    "meaningful_words = [w for w in text if not w in stops]\n",
    "print(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('review.txt','r',encoding='UTF8')\n",
    "text=file.readlines()\n",
    "#print(text)\n",
    "file.close()\n",
    "tex=''\n",
    "for i in range(len(text)):\n",
    "    tex+=text[i].strip()\n",
    "text=tex\n",
    "text=okt.pos(text,norm=True,stem=True)\n",
    "for i in range(len(text)):\n",
    "    text[i]=text[i][0]\n",
    "stop_words = \"? 까지 전에 죠 란 만 해도 하지만 조금 아마 기필코 이후 이미 일 『  』 것 이다 이는 들 세요 알다 면 연 이니까 이기에 수 와 과 에 기 작 있다 도 을 를 오전 오후 오늘 내일 그제 이름 이 그 저 않습니다 왜냐하면 되기 를 은 , . 노 하 하는 이 가 은 는 하는 왜냐하면 하기 에서 라는 의 하다 않다 또\" \n",
    "stop_words=stop_words.split(' ') \n",
    "stops=set(stop_words)\n",
    "meaningful_words = [w for w in text if not w in stops]\n",
    "#print(meaningful_words)\n",
    "meaningful_words=review_to_words(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('review.txt','r',encoding='UTF8')\n",
    "text=file.readlines()\n",
    "file.close()\n",
    "tex=''\n",
    "for i in range(len(text)):\n",
    "    tex+=text[i].strip()\n",
    "text=tex\n",
    "text=okt.phrases(text)\n",
    "stop_words = \"? 까지 전에 죠 란 만 해도 하지만 조금 아마 기필코 이후 이미 일 『  』 것 이다 이는 들 세요 알다 면 연 이니까 이기에 수 와 과 에 기 작 있다 도 을 를 오전 오후 오늘 내일 그제 이름 이 그 저 않습니다 왜냐하면 되기 를 은 , . 노 하 하는 이 가 은 는 하는 왜냐하면 하기 에서 라는 의 하다 않다 또\" \n",
    "stop_words=stop_words.split(' ') \n",
    "stops=set(stop_words)\n",
    "meaningful_words = [w for w in text if not w in stops]\n",
    "#print(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-de8d64f18095>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m model = word2vec.Word2Vec(meaningful_words, \n\u001b[0m\u001b[0;32m     15\u001b[0m                           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                           \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "# 파라메터값 지정\n",
    "num_features = 50 # 문자 벡터 차원 수\n",
    "min_word_count = 2 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context =5 # 문자열 창 크기\n",
    "downsampling =  1e-3  # 문자 빈도 수 Downsample\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(meaningful_words, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "model\n",
    "model.wv.vectors.shape\n",
    "model_name = \"feature100_context2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('하늘', topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 참고 https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim \n",
    "import gensim.models as g\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "print(len(X))\n",
    "print(X[0][:10])\n",
    "tsne = TSNE()\n",
    "\n",
    "# 100개의 단어에 대해서만 시각화\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "# X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(40, 20)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
