{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3313d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e74bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBoWModel 선언부\n",
    "class CBowModel(object):\n",
    "    \n",
    "    def __init__(self, train_fname,embedding_fname,\n",
    "               model_fname, embeddign_corpus_fname,\n",
    "               embedding_metthod=\"fasttext\", is_weighted=True,\n",
    "               average=False,dim=100, tokenizer='twitter'):\n",
    "        #configurations\n",
    "        make_save_path(model_fname)\n",
    "        self.dim=dim\n",
    "        self.average = average\n",
    "        if is_weighted:\n",
    "            model_full_fname = model_fname + \"-weighted\"\n",
    "        else :\n",
    "            model_full_fname = model_fname + \"-original\"\n",
    "        \n",
    "        self.tokenizer = get_tokenizer(tokenizer_name)\n",
    "        if is_weighted:\n",
    "            self.embeddings = \\\n",
    "                self.load_or_construct_weighted(embedding_fname,\n",
    "                                               embedding_method,embedding_corpus_fname)\n",
    "            print(\"loading weighted embeddings, complete!\")\n",
    "        else:\n",
    "            #original embeddings\n",
    "            words, vectors = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "            self.embeddings = defaultdict(list)\n",
    "            for word, vector in zip(words,vectors):\n",
    "                self.embedding[word] = vector\n",
    "            print(\"loading original embeddings, complete!\")\n",
    "        if not os.path.exists(model_full_fname):\n",
    "            print(\"train Continuous Bag of Words model\")\n",
    "            self.model = self.train_model(train_fname, model_full_name)\n",
    "        else:\n",
    "            print(\"load Continuous Bag of Words model\")\n",
    "            self.model = self.load_model(model_full_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBoWModel 임베딩 학습 말뭉치 내 단어별 빈도 확인\n",
    "def compute_word_frequency(self,embedding_corpus_fname):\n",
    "        total_count = 0\n",
    "        words_count = defaultdict(int)\n",
    "        with open(embedding_corpus_fname,\"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line,strip().split()\n",
    "                for token in tokens:\n",
    "                    words_count[token]+=1\n",
    "                    total_count +=1\n",
    "        return words_count, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBoWModel 가중 임베딩 만들기\n",
    "def load_or_construct_weighted_embedding(self, embedding_fname,\n",
    "                                        embedding_method,\n",
    "                                        embedding_corpus_fname,a=0.001):\n",
    "    dictionary = {}\n",
    "    if os.path.exists(embedding_fname + \"-weighted\"):\n",
    "        # load weighted word embeddings\n",
    "        with open(embedding_fname+\"-weighted\",\"r\") as f2:\n",
    "            for line in f2:\n",
    "                word, weighted_vector = line.strip().split(\"\\u241E\")\n",
    "                weighted_vector = [float(el) for el in weighted_vector.split()]\n",
    "                dictionary[word] = weighted_vector\n",
    "    else:\n",
    "        #load pretrained word embeddings\n",
    "        words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "        \n",
    "        #compute word frequency\n",
    "        words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname)\n",
    "        \n",
    "        #construct weighted word embeddings\n",
    "        with open(embedding_fname + \"-weighted\",\"w\") as f3:\n",
    "            for word, vsc in zip (words, vecs):\n",
    "                if word in words_count.keys():\n",
    "                    word_prob = words_count[word]/total_word_count\n",
    "                else:\n",
    "                    word_prob = 0.0\n",
    "                weighted_vector = (a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
