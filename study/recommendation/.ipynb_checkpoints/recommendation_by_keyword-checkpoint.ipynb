{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드 기반 추천시스템 recommendation_by_keyword\n",
    " - bookdata.csv에 있는 title, review을 활용하여 keyword을 입력하면 그 keyword들의 평균벡터와 가장 벡터값이 유사한 책을 추천\n",
    " - 작동 설명\n",
    "     1. review에 있는 데이터들로 임베딩 모델을 만든 뒤 책마다 특정 벡터값을 가질 수 있도록 토큰화된 단어들의 벡터값의 평균을 낸다.\n",
    "     2. 키워드를 인자로 입력받으면 그 키워드의 평균 벡터값을 내 문서 벡터마다 코사인 유사도를 구한다\n",
    "     3. 구한 코사인 유사도가 높은 순서대로 5개의 책을 추천한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ceNy5bBEO_R"
   },
   "source": [
    "1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qCQB8aVW2FFk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (3.7.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim==3.7.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim==3.7.0) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim==3.7.0) (1.20.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim==3.7.0) (1.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim==3.7.0) (2.26.0)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\user\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim==3.7.0) (2.49.0)\n",
      "Requirement already satisfied: bz2file in c:\\users\\user\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim==3.7.0) (0.98)\n",
      "Requirement already satisfied: boto3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim==3.7.0) (1.17.88)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim==3.7.0) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.88 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim==3.7.0) (1.20.94)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim==3.7.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from botocore<1.21.0,>=1.20.88->boto3->smart-open>=1.7.0->gensim==3.7.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from botocore<1.21.0,>=1.20.88->boto3->smart-open>=1.7.0->gensim==3.7.0) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim==3.7.0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim==3.7.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim==3.7.0) (2.6)\n"
     ]
    }
   ],
   "source": [
    "# 패키지 임포트\n",
    "\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "import io\n",
    "from io import BytesIO\n",
    "!pip install gensim==3.7.0 --user\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b6RdQ_29Fg31"
   },
   "outputs": [],
   "source": [
    "# 데이터를 데이터프레임으로 로드하고, 전체 문서의 수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pSDzOOLFGLi_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서의 수 :  139\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bookdata.csv')\n",
    "print('전체 문서의 수 : ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s2K_HB98Gc30"
   },
   "outputs": [],
   "source": [
    "# 상위 5행만 출력하여 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I6o4v8-II4Oo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>달러구트 꿈 백화점 2</td>\n",
       "      <td>이미예</td>\n",
       "      <td>9791165343729</td>\n",
       "      <td>달러구트 꿈백화점 1권을 읽으신 분들이라면 2권을 손꼽아 기다렸을 것이다. 나 또한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>달러구트 꿈 백화점</td>\n",
       "      <td>이미예</td>\n",
       "      <td>9791165341909</td>\n",
       "      <td>평소에 책을 너무 안 읽는 것 같아 계속 베스트셀러에 올라와 있기에 주문해 보았습니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>미드나잇 라이브러리</td>\n",
       "      <td>매트 헤이그</td>\n",
       "      <td>9791191056556</td>\n",
       "      <td>죽음이라는 주제로 어둡게 이야기가 흘러가지는 않을까 걱정했는데 죽음보다는 인생에 대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>오늘 밤, 세계에서 이 사랑이 사라진다 해도</td>\n",
       "      <td>이치조 미사키</td>\n",
       "      <td>9791191043297</td>\n",
       "      <td>우연히 SNS에서 이 책의 줄거리를 읽게 되었습니다. 이런 표지의 감성을 담은 소설...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>완전한 행복</td>\n",
       "      <td>정유정</td>\n",
       "      <td>9791167370280</td>\n",
       "      <td>무서워서 낮에만 읽으려고 했던 나의 결심?다짐?을 반달늪에 쳐박아 버린 소설. 정유...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title   author         isbn13  \\\n",
       "0              달러구트 꿈 백화점 2      이미예  9791165343729   \n",
       "1                달러구트 꿈 백화점      이미예  9791165341909   \n",
       "2                미드나잇 라이브러리   매트 헤이그  9791191056556   \n",
       "3  오늘 밤, 세계에서 이 사랑이 사라진다 해도  이치조 미사키  9791191043297   \n",
       "4                    완전한 행복      정유정  9791167370280   \n",
       "\n",
       "                                              review  \n",
       "0  달러구트 꿈백화점 1권을 읽으신 분들이라면 2권을 손꼽아 기다렸을 것이다. 나 또한...  \n",
       "1  평소에 책을 너무 안 읽는 것 같아 계속 베스트셀러에 올라와 있기에 주문해 보았습니...  \n",
       "2  죽음이라는 주제로 어둡게 이야기가 흘러가지는 않을까 걱정했는데 죽음보다는 인생에 대...  \n",
       "3  우연히 SNS에서 이 책의 줄거리를 읽게 되었습니다. 이런 표지의 감성을 담은 소설...  \n",
       "4  무서워서 낮에만 읽으려고 했던 나의 결심?다짐?을 반달늪에 쳐박아 버린 소설. 정유...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석 하기 전 사용자 사전 추가\n",
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.add_dictionary(list(df['author']),'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jfl6jZPMI6Mv"
   },
   "outputs": [],
   "source": [
    "# 'Desc' 열을 대상으로 전처리를 수행하여 'cleaned'에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kme9jG6SJXi1"
   },
   "outputs": [],
   "source": [
    "def _removeNonAscii(text):\n",
    "    letters_only = re.sub('[^ 0-9가-힣]', '', text)   \n",
    "    return letters_only\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    twitter = Twitter()\n",
    "    tokenized_data = []\n",
    "    temp_X = twitter.nouns(text) # 토큰화\n",
    "    \n",
    "    # 5. Stopwords 불용어 제거    \n",
    "    file=open('bool.txt','r',encoding='UTF8')\n",
    "    stopwords=file.readlines()[0].split(' ')\n",
    "    file.close()\n",
    "    \n",
    "    temp_X = [word for word in temp_X if not word in stopwords] \n",
    "    tokenized_data.append(temp_X)\n",
    "    return( ' '.join(temp_X) )\n",
    "\n",
    "df['cleaned'] = df['review'].apply(_removeNonAscii)\n",
    "df['cleaned'] = df.cleaned.apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cRzzXt7JJp2b"
   },
   "outputs": [],
   "source": [
    "# 상위 5행만 출력하여 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QuB-mIfTKUGk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    달러 구트 꿈 백화점 권 분 라면 권 권 꿈 꿈 복권 꿈 자리 악몽 식은땀 다양 꿈...\n",
       "1    평소 책 계속 베스트셀러 주문 판타지 내용 그냥 다가 계속 끝 부담 꿈 사고 판다 ...\n",
       "2    죽음 주제 이야기 걱정 죽음 인생 루고 실제 죽음 삶 사이 경계 선 보기 주인공 삶...\n",
       "3    우연 책 줄거리 표지 감성 소설 취향 줄거리 한번 보고 내용 궁금 구매 기억 남자 ...\n",
       "4    낮 결심 다짐 반달 늪 박아 소설 정유 정 작가 소설 장소 버 린다 고 일어 모든 ...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Vrpk6GbHKZs1"
   },
   "outputs": [],
   "source": [
    "# 빈 값이 있는 행 확인, nan 값으로 변환 후 해당 행 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r8zUEHJkKu8J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서의 수 : 139\n"
     ]
    }
   ],
   "source": [
    "df['cleaned'].replace('', np.nan, inplace=True)\n",
    "df = df[df['cleaned'].notna()]\n",
    "print('전체 문서의 수 :', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eQuXSeTeLDiX"
   },
   "outputs": [],
   "source": [
    "# 토큰화하여 corpus 라는 리스트에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CqKh2P9LLeex"
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for words in df['cleaned']:\n",
    "    corpus.append(words.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xopTwXK9L2X2"
   },
   "source": [
    "2. Word2Vec을 통해 이를 초기 단어 벡터값으로 워드 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kXxsGVwgMHTF"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, size = 300, window=5, min_count = 5, workers = -1)\n",
    "#word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True)\n",
    "#word2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY5JSryLMcr3"
   },
   "source": [
    "3. 단어 벡터의 평균 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-6bc9cd06868f>:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  doc2vec = word2vec_model[word]\n",
      "<ipython-input-17-6bc9cd06868f>:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  doc2vec = doc2vec + word2vec_model[word]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 벡터의 수 : 139\n"
     ]
    }
   ],
   "source": [
    "word2vec_model=model\n",
    "\n",
    "def vectors(document_list):\n",
    "    document_embedding_list = []\n",
    "\n",
    "    # 각 문서에 대해서\n",
    "    for line in document_list:\n",
    "        doc2vec = None\n",
    "        count = 0\n",
    "        for word in line.split():\n",
    "            if word in word2vec_model.wv.vocab.keys():\n",
    "                count += 1\n",
    "                # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.\n",
    "                if doc2vec is None:\n",
    "                    doc2vec = word2vec_model[word]\n",
    "                else:\n",
    "                    doc2vec = doc2vec + word2vec_model[word]\n",
    "\n",
    "        if doc2vec is not None:\n",
    "            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\n",
    "            doc2vec = doc2vec / count\n",
    "            document_embedding_list.append(doc2vec)\n",
    "\n",
    "    # 각 문서에 대한 문서 벡터 리스트를 리턴\n",
    "    return document_embedding_list \n",
    "\n",
    "document_embedding_list = vectors(df['cleaned'])\n",
    "print('문서 벡터의 수 :',len(document_embedding_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lmj8EOQfMjoJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def keyword_vector(keyword_list):    \n",
    "    # 각 키워드에 대해서\n",
    "    key2vec = None\n",
    "    count = 0\n",
    "    \n",
    "    for word in keyword_list:\n",
    "        if word in word2vec_model.wv.vocab.keys():\n",
    "                count += 1\n",
    "                # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.\n",
    "                if key2vec is None:\n",
    "                    key2vec = word2vec_model[word]\n",
    "                else:\n",
    "                    key2vec = key2vec + word2vec_model[word]\n",
    "\n",
    "    if key2vec is not None:\n",
    "            # 단어 벡터를 모두 더한 벡터의 값을 키워드 개수로 나눠준다.\n",
    "        key2vec = key2vec / count\n",
    "            \n",
    "    # 각 문서에 대한 문서 벡터 리스트를 리턴\n",
    "    return key2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqGzBoTlNmmK"
   },
   "source": [
    "4. 추천 시스템 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tYc6qvggNyp7"
   },
   "outputs": [],
   "source": [
    "# 코사인 유사도를 이용하여, 가장 줄거리가 유사한 5개의 책을 찾아내는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "\n",
    "def recommendation_by_keyword(keyword_list):\n",
    "    \n",
    "    keyVector=keyword_vector(keyword_list)\n",
    "    \n",
    "    books = df[['title']]\n",
    "    \n",
    "    cosine_similarities2=[]\n",
    "    for doc in document_embedding_list:\n",
    "        cosine_similarities2.append(cos_sim(keyVector, np.array(doc)))\n",
    "    \n",
    "\n",
    "    # 입력된 키워드의 평균 벡트와 코사인 유사도가 가장 큰 책 5개 선정.\n",
    "    sim_scores = list(enumerate(cosine_similarities2))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "\n",
    "    # 가장 유사한 책 5권의 인덱스\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 전체 데이터프레임에서 해당 인덱스의 행만 추출. 5개의 행을 가진다.\n",
    "    recommend = books.iloc[book_indices].reset_index(drop=True)\n",
    "\n",
    "    # 데이터프레임으로부터 순차적으로 제목을 출력\n",
    "    for i in book_indices:\n",
    "        print(books.loc[i]['title']) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6pWH67rGONNa"
   },
   "outputs": [],
   "source": [
    "# 확인 : 책 제목을 입력하면 이와 유사한 책들 추천(제목, 표지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기억 2\n",
      "유성의 인연 1\n",
      "나무를 심은 사람\n",
      "유성의 인연 2\n",
      "아오이가든\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-42c2dccf3232>:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  key2vec = word2vec_model[word]\n"
     ]
    }
   ],
   "source": [
    "recommendation_by_keyword(['공포','암울'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삶의 한가운데\n",
      "바늘과 가죽의 시\n",
      "사장님, 아무거나 먹지 마세요\n",
      "너는 기억 못하겠지만\n",
      "심판\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-42c2dccf3232>:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  key2vec = word2vec_model[word]\n",
      "<ipython-input-18-42c2dccf3232>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  key2vec = key2vec + word2vec_model[word]\n"
     ]
    }
   ],
   "source": [
    "recommendation_by_keyword(['삶','죽음'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기억 2\n",
      "그리스인 조르바\n",
      "유성의 인연 1\n",
      "한국단편소설 70\n",
      "두근두근 내 인생\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-42c2dccf3232>:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  key2vec = word2vec_model[word]\n",
      "<ipython-input-18-42c2dccf3232>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  key2vec = key2vec + word2vec_model[word]\n"
     ]
    }
   ],
   "source": [
    "recommendation_by_keyword(['공포','스릴러'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리가 함께 장마를 볼 수도 있겠습니다\n",
      "달러구트 꿈 백화점 2\n",
      "달러구트 꿈 백화점\n",
      "사랑하라 한번도 상처받지 않은 것처럼\n",
      "나는 내일, 어제의 너와 만난다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-42c2dccf3232>:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  key2vec = word2vec_model[word]\n",
      "<ipython-input-18-42c2dccf3232>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  key2vec = key2vec + word2vec_model[word]\n"
     ]
    }
   ],
   "source": [
    "recommendation_by_keyword(['로맨스','감동'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlp_1011_recommendation system using document embedding 문서벡터를 이용한 추천.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
